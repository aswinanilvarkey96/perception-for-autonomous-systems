{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly project\n",
    "\n",
    "Today you are going to implement the last parts of the algorithm you started on monday. For reference you can see it below.\n",
    "\n",
    "![title](algorithm_3.png)\n",
    "\n",
    "It is a good idea to follow and track the steps in the algorithm in the below implementation. Only take one step at a time.\n",
    "\n",
    "Once you have the algorithm up and running you can try with a larger dataset to see if your algorithm is able to maintain good accurracy over a longer distance. The larger dataset can be found here:\n",
    "[Left images](https://dtudk-my.sharepoint.com/:u:/g/personal/evanb_dtu_dk/EQu8kmGBDDROtGJ7IkZB2tQBJrxmgY9t8LVM_JuEi83TYw)\n",
    "[Right images](https://dtudk-my.sharepoint.com/:u:/g/personal/evanb_dtu_dk/EcKI_zrXTvpMulizidCZm4oBLJcQ_LTV9Zs6oQFF74JTRQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge \n",
    "The current implementation only uses features computed at the current timestep. However, as we process more images we potentially have a lot of features from previous timesteps that are still valid. The challenge is to expand the `extract_keypoints_surf(..., refPoints)` function by giving it old reference points. You should then combine your freshly computed features with the old features and remove all duplicates. This requires you to keep track of old features and 3D points.\n",
    "\n",
    "Hint 1: look in `helpers.py` for removing duplicates.\n",
    "\n",
    "Hint 2: you are not interested in points that are behind you, so remember to remove points that are negative in the direction you move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.xfeatures2d' has no attribute 'SIFT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly project.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=330'>331</a>\u001b[0m baseline \u001b[39m=\u001b[39m \u001b[39m0.54\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=331'>332</a>\u001b[0m K \u001b[39m=\u001b[39m  getK()\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=333'>334</a>\u001b[0m playImageSequence(left_img, right_img, K)\n",
      "\u001b[1;32m/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly project.ipynb Cell 4'\u001b[0m in \u001b[0;36mplayImageSequence\u001b[0;34m(left_img, right_img, K)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=215'>216</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=216'>217</a>\u001b[0m \u001b[39m    different ways to initialize the query points and landmark points\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=217'>218</a>\u001b[0m \u001b[39m    you can specify the keypoints and landmarks\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=218'>219</a>\u001b[0m \u001b[39m    or you can inilize_3D with FAST corner points, then stere match and then generate 3D points, but not so accurate\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=219'>220</a>\u001b[0m \u001b[39m    or you can use the OPENCV feature extraction and matching functions\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=220'>221</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=222'>223</a>\u001b[0m \u001b[39m#p1 = getKepoints().astype('float32')\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=223'>224</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=224'>225</a>\u001b[0m \u001b[39m#print(p1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=231'>232</a>\u001b[0m \u001b[39m# print(points.shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=232'>233</a>\u001b[0m \u001b[39m# print(p1.shape)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=234'>235</a>\u001b[0m points, p1 \u001b[39m=\u001b[39m extract_keypoints_surf(left_img, right_img, K, baseline)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=235'>236</a>\u001b[0m p1 \u001b[39m=\u001b[39m p1\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=237'>238</a>\u001b[0m pnp_objP \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(points, axis \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m)\n",
      "\u001b[1;32m/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly project.ipynb Cell 4'\u001b[0m in \u001b[0;36mextract_keypoints_surf\u001b[0;34m(img1, img2, K, baseline, refPoints)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=168'>169</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_keypoints_surf\u001b[39m(img1, img2, K, baseline, refPoints \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=170'>171</a>\u001b[0m     detector \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mxfeatures2d\u001b[39m.\u001b[39;49mSIFT(\u001b[39m400\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=171'>172</a>\u001b[0m     kp1, desc1 \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39mdetectAndCompute(img1, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/JerzyNawrocki/projekty/perception/perception-for-autonomous-systems/notebooks/week_9/project/weekly%20project.ipynb#ch0000003vscode-remote?line=172'>173</a>\u001b[0m     kp2, desc2 \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39mdetectAndCompute(img2,\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2.xfeatures2d' has no attribute 'SIFT'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from numpy.linalg import inv, pinv\n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getK():\n",
    "    return np.array([[7.188560e+02, 0.000000e+00, 6.071928e+02],\n",
    "                     [0, 7.188560e+02, 1.852157e+02],\n",
    "                     [0, 0, 1]])\n",
    "\n",
    "def getTruePose():\n",
    "    file = '/jupyter/notebooks/week_9/project/00.txt'\n",
    "    return np.genfromtxt(file, delimiter=' ', dtype=None)\n",
    "\n",
    "def getLeftImage(i):\n",
    "    # get absolute path\n",
    "    path = os.getcwd()\n",
    "    return cv2.imread('/jupyter/notebooks/week_9/project/left/{0:010d}.png'.format(i), 0)\n",
    "\n",
    "def getRightImage(i):\n",
    "    return cv2.imread('/jupyter/notebooks/week_9/project/right/{0:010d}.png'.format(i), 0)\n",
    "\n",
    "def featureDetection(img, numCorners):\n",
    "\n",
    "    h, w   = img.shape\n",
    "\n",
    "    thresh = dict(threshold=24, nonmaxSuppression=True)\n",
    "    fast   = cv2.FastFeatureDetector_create(**thresh)\n",
    "    kp1    = fast.detect(img)\n",
    "    kp1    = sorted(kp1, key = lambda x:x.response, reverse=True)[:numCorners]\n",
    "\n",
    "    p1     = np.array([ele.pt for ele in kp1],dtype='int')\n",
    "    # img3 = cv2.drawKeypoints(img, kp1, None, color=(255,0,0))\n",
    "    # cv2.imshow('fast',img3)\n",
    "    # cv2.waitKey(0) & 0xFF\n",
    "    return p1\n",
    "\n",
    "def featureTracking(img_1, img_2, p1, world_points):\n",
    "    ##use KLT tracker\n",
    "    lk_params = dict( winSize  = (21,21),\n",
    "                      maxLevel = 3,\n",
    "                      criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.01))\n",
    "\n",
    "    p2, st, err = cv2.calcOpticalFlowPyrLK(img_1, img_2, p1, None, **lk_params)\n",
    "    st = st.reshape(st.shape[0])\n",
    "    ##find good one\n",
    "    pre = p1[st==1]\n",
    "    p2 = p2[st==1]\n",
    "    w_points  = world_points[st==1]\n",
    "\n",
    "    return w_points, pre,p2\n",
    "\n",
    "\n",
    "def stereo_match_feature(left_img, right_img, patch_radius, keypoints, min_disp, max_disp):  \n",
    "    # in case you want to find stereo match by yourself\n",
    "    h, w = left_img.shape\n",
    "    num_points = keypoints.shape[0]\n",
    "\n",
    "    # Depth (or disparity) map\n",
    "    depth = np.zeros(left_img.shape, np.uint8)\n",
    "    output = np.zeros(keypoints.shape, dtype='int')\n",
    "    all_index = np.zeros((keypoints.shape[0],1), dtype='int').reshape(-1)\n",
    "\n",
    "    r     = patch_radius\n",
    "    # patch_size = 2*patch_radius + 1;\n",
    "      \n",
    "    for i in range(num_points):\n",
    "\n",
    "        row, col = keypoints[i,0], keypoints[i,1]\n",
    "        # print(row, col)\n",
    "        best_offset = 0;\n",
    "        best_score = float('inf');\n",
    "\n",
    "        if (row-r < 0 or row + r >= h or col - r < 0 or col + r >= w): continue\n",
    "\n",
    "        left_patch = left_img[(row-r):(row+r+1), (col-r):(col+r+1)] # left imag patch    \n",
    "\n",
    "        all_index[i] = 1\n",
    "\n",
    "        for offset in range(min_disp, max_disp+1):\n",
    "\n",
    "              if (row-r) < 0 or row + r >= h or  (col-r-offset) < 0 or (col+r-offset) >= w: continue\n",
    "        \n",
    "              diff  = left_patch - right_img[(row-r):(row+r+1), (col-r-offset):(col+r-offset+1)]\n",
    "              sum_s = np.sum(diff**2)\n",
    " \n",
    "              if sum_s < best_score:\n",
    "                  best_score = sum_s\n",
    "                  best_offset = offset\n",
    "\n",
    "        output[i,0], output[i,1] = row,col-best_offset\n",
    "\n",
    "    return output, all_index\n",
    "\n",
    "\n",
    "def generate3D(featureL, featureR, K, baseline):\n",
    "        # points should be 3xN and intensities 1xN, where N is the amount of pixels\n",
    "        # which have a valid disparity. I.e., only return points and intensities\n",
    "        # for pixels of left_img which have a valid disparity estimate! The i-th\n",
    "        # intensity should correspond to the i-th point.\n",
    "\n",
    "        temp = featureL - featureR\n",
    "        temp = temp[:,1]\n",
    "\n",
    "        print(featureL.shape, featureR.shape)\n",
    "        \n",
    "        px_left  =  np.vstack((featureL.T, np.ones((1, featureL.shape[0]))))\n",
    "        # Switch from (row, col, 1) to (u, v, 1)\n",
    "        px_left[0:2, :] = np.flipud(px_left[0:2, :])\n",
    "\n",
    "        bv_left = inv(K).dot(px_left)\n",
    "\n",
    "        f = K[0,0]\n",
    "\n",
    "        z = f*baseline/temp\n",
    "        points = bv_left*z\n",
    "\n",
    "        #intensities = left_img.reshape(-1)[disp_im > 0]\n",
    "\n",
    "        return points\n",
    "\n",
    "\n",
    "def removeDuplicate(queryPoints, refPoints, radius=5):\n",
    "    #remove duplicate points from new query points,\n",
    "    for i in range(len(queryPoints)):\n",
    "        query = queryPoints[i]\n",
    "        xliml, xlimh = query[0]-radius, query[0]+radius\n",
    "        yliml, ylimh = query[1]-radius, query[1]+radius\n",
    "        inside_x_lim_mask = (refPoints[:,0] > xliml) & (refPoints[:,0] < xlimh)\n",
    "        curr_kps_in_x_lim = refPoints[inside_x_lim_mask]\n",
    "\n",
    "        if curr_kps_in_x_lim.shape[0] != 0:\n",
    "            inside_y_lim_mask = (curr_kps_in_x_lim[:,1] > yliml) & (curr_kps_in_x_lim[:,1] < ylimh)\n",
    "            curr_kps_in_x_lim_and_y_lim = curr_kps_in_x_lim[inside_y_lim_mask,:]\n",
    "            if curr_kps_in_x_lim_and_y_lim.shape[0] != 0:\n",
    "                queryPoints[i] =  np.array([0,0])\n",
    "    return (queryPoints[:, 0]  != 0 )\n",
    "\n",
    "\n",
    "def initiliazatize_3D_points(left_img, right_img, K, baseline):\n",
    "\n",
    "    p1 = featureDetection(left_img, 500)\n",
    "\n",
    "    p1 = np.fliplr(p1)\n",
    "\n",
    "    #img_show  = cv2.imread('../data/left/{0:06d}.png'.format(0))\n",
    "\n",
    "    p2, all_index = stereo_match_feature(left_img, right_img, 5, p1, 5, 50)\n",
    "\n",
    "    p1 = p1[all_index > 0, :]\n",
    "    p2 = p2[all_index > 0, :]\n",
    "\n",
    "    M_left = K.dot(np.hstack((np.eye(3), np.zeros((3,1)))))\n",
    "\n",
    "    M_rght = K.dot(np.hstack((np.eye(3), np.array([[-baseline,0, 0]]).T)))\n",
    "\n",
    "    p1_flip = np.vstack((np.flipud(p1.T),np.ones((1,p1.shape[0]))))\n",
    "    p2_flip    = np.vstack((np.flipud(p2.T),np.ones((1,p1.shape[0]))))\n",
    "\n",
    "    # for p in p1:\n",
    "    #     cv2.circle(img_show, (p[1], p[0]) ,1, (0,0,255), 2);\n",
    "\n",
    "    P = cv2.triangulatePoints(M_left, M_rght, p1_flip[:2], p2_flip[:2]) \n",
    "\n",
    "    P = P/P[3]\n",
    "    points = P[:3]\n",
    "\n",
    "    # for p in p1:\n",
    "    #     cv2.circle(left_img, (p[0], p[1]) ,1, (0,0,255), 2);\n",
    "\n",
    "    # cv2.imshow('images', img_show)\n",
    "    # k = cv2.waitKey(0) & 0xFF\n",
    "    #print(points.T)\n",
    "    return points.T, p1\n",
    "\n",
    "def extract_keypoints_surf(img1, img2, K, baseline, refPoints = None):\n",
    "\n",
    "    # sift function to detect keypoints\n",
    "    surf = cv2.SIFT_create()\n",
    "\n",
    "    #surf = cv2.xfeatures2d.SURF_create(400)\n",
    "    kp1, desc1 = surf.detectAndCompute(img1, None)\n",
    "    kp2, desc2 = surf.detectAndCompute(img2, None)\n",
    "\n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict()   # or pass empty dictionary\n",
    "    flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "    matches = flann.knnMatch(desc1,desc2,k=2)\n",
    "\n",
    "    # ratio test as per Lowe's paper\n",
    "    match_points1, match_points2 = [], []\n",
    "    for i,(m,n) in enumerate(matches):\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            match_points1.append(kp1[m.queryIdx].pt)\n",
    "            match_points2.append(kp2[m.trainIdx].pt)\n",
    "\n",
    "    print('old lengthL', len(match_points1))\n",
    "\n",
    "    p1 = np.array(match_points1).astype(float)\n",
    "    p2 = np.array(match_points2).astype(float)\n",
    "\n",
    "    if refPoints is not None:\n",
    "        mask = removeDuplicate(p1, refPoints)\n",
    "        p1 = p1[mask,:]\n",
    "        p2 = p2[mask,:]\n",
    "\n",
    "    print('new lengthL ', len(p1))\n",
    "\n",
    "    M_left = K.dot(np.hstack((np.eye(3), np.zeros((3,1)))))\n",
    "\n",
    "    M_rght = K.dot(np.hstack((np.eye(3), np.array([[-baseline,0, 0]]).T)))\n",
    "\n",
    "    p1_flip = np.vstack((p1.T,np.ones((1,p1.shape[0]))))\n",
    "    p2_flip = np.vstack((p2.T,np.ones((1,p2.shape[0]))))\n",
    "\n",
    "    P = cv2.triangulatePoints(M_left, M_rght, p1_flip[:2], p2_flip[:2]) \n",
    "\n",
    "    P = P/P[3]\n",
    "    land_points = P[:3]\n",
    "\n",
    "    return land_points.T, p1\n",
    "\n",
    "def playImageSequence(left_img, right_img, K):\n",
    "    '''\n",
    "        different ways to initialize the query points and landmark points\n",
    "        you can specify the keypoints and landmarks\n",
    "        or you can inilize_3D with FAST corner points, then stere match and then generate 3D points, but not so accurate\n",
    "        or you can use the OPENCV feature extraction and matching functions\n",
    "    '''\n",
    "\n",
    "    #p1 = getKepoints().astype('float32')\n",
    "\n",
    "    #print(p1)\n",
    "\n",
    "    #points = getLandMarks()\n",
    "\n",
    "    # points, p1 = initiliazatize_3D_points(left_img, right_img, K, baseline)\n",
    "    # points = points.T\n",
    "    #p1 = np.fliplr(p1).astype('float32')\n",
    "    # print(points.shape)\n",
    "    # print(p1.shape)\n",
    "\n",
    "    points, p1 = extract_keypoints_surf(left_img, right_img, K, baseline)\n",
    "    p1 = p1.astype('float32')\n",
    "\n",
    "    pnp_objP = np.expand_dims(points, axis = 2)\n",
    "    pnp_p1   = np.expand_dims(p1, axis = 2).astype(float)\n",
    "\n",
    "    # reference\n",
    "    reference_img = left_img\n",
    "    reference_2D  = p1\n",
    "    landmark_3D   = points\n",
    "\n",
    "    #_, rvec, tvec = cv2.solvePnP(pnp_objP, pnp_p1, K, None)\n",
    "    truePose = getTruePose()\n",
    "\n",
    "    traj = np.zeros((600, 600, 3), dtype=np.uint8);\n",
    "\n",
    "    maxError = 0\n",
    "\n",
    "    for i in range(0, 101):\n",
    "            print('image: ', i)\n",
    "            curImage =  getLeftImage(i)\n",
    "           # curImage = cv2.imread('../data/left/{0:06d}.png'.format(i), 0)\n",
    "\n",
    "            landmark_3D, reference_2D, tracked_2Dpoints = featureTracking(reference_img, curImage, reference_2D,  landmark_3D)\n",
    "\n",
    "            # print(len(landmark_3D), len(valid_land_mark))\n",
    "\n",
    "            pnp_objP = np.expand_dims(landmark_3D, axis = 2)\n",
    "            pnp_cur  = np.expand_dims(tracked_2Dpoints, axis = 2).astype(float)\n",
    "\n",
    "            _, rvec, tvec, inliers = cv2.solvePnPRansac(pnp_objP , pnp_cur, K, None)\n",
    "\n",
    "            #update the new reference_2D\n",
    "            reference_2D = tracked_2Dpoints[inliers[:,0],:]\n",
    "            landmark_3D  = landmark_3D[inliers[:,0],:]\n",
    "            \n",
    "            #retrieve the rotation matrix\n",
    "            rot,_ = cv2.Rodrigues(rvec)\n",
    "            tvec = -rot.T.dot(tvec)     #coordinate transformation, from camera to world\n",
    "\n",
    "            inv_transform = np.hstack((rot.T,tvec)) #inverse transform\n",
    "\n",
    "            inliers_ratio = len(inliers)/len(pnp_objP) # the inlier ratio\n",
    "\n",
    "            print('inliers ratio: ',inliers_ratio)\n",
    "\n",
    "            # re-obtain the 3 D points if the conditions satisfied\n",
    "            if (inliers_ratio < 0.9 or len(reference_2D) < 50):\n",
    "\n",
    "                    ##initiliazation new landmarks\n",
    "                    curImage_R = getRightImage(i)\n",
    "                    # landmark_3D, reference_2D = initiliazatize_3D_points(curImage, curImage_R, K, baseline)\n",
    "                    # reference_2D = np.fliplr(reference_2D).astype('float32')\n",
    "                    landmark_3D_new, reference_2D_new  = extract_keypoints_surf(curImage, curImage_R, K, baseline, reference_2D)\n",
    "                    reference_2D_new = reference_2D_new.astype('float32')\n",
    "                    landmark_3D_new = inv_transform.dot(np.vstack((landmark_3D_new.T, np.ones((1,landmark_3D_new.shape[0])))))\n",
    "                    valid_matches = landmark_3D_new[2,:] >0\n",
    "                    landmark_3D_new = landmark_3D_new[:,valid_matches]\n",
    "\n",
    "                    reference_2D = np.vstack((reference_2D, reference_2D_new[valid_matches,:]))\n",
    "                    landmark_3D =  np.vstack((landmark_3D, landmark_3D_new.T))\n",
    "        \n",
    "            reference_img = curImage\n",
    "\n",
    "            #draw images\n",
    "            draw_x, draw_y = int(tvec[0]) + 300, int(tvec[2]) + 100;\n",
    "            true_x, true_y = int(truePose[i][3]) + 300, int(truePose[i][11]) + 100\n",
    "\n",
    "            curError = np.sqrt((tvec[0]-truePose[i][3])**2 + (tvec[1]-truePose[i][7])**2 + (tvec[2]-truePose[i][11])**2)\n",
    "            print('Current Error: ', curError)\n",
    "            if (curError > maxError):\n",
    "                maxError = curError\n",
    "\n",
    "            # print([truePose[i][3], truePose[i][7], truePose[i][11]])\n",
    "\n",
    "            text = \"Coordinates: x ={0:02f}m y = {1:02f}m z = {2:02f}m\".format(float(tvec[0]), float(tvec[1]), float(tvec[2]));\n",
    "            cv2.circle(traj, (draw_x, draw_y) ,1, (0,0,255), 2);\n",
    "            cv2.circle(traj, (true_x, true_y) ,1, (255,0,0), 2);\n",
    "            cv2.rectangle(traj, (10, 30), (550, 50), (0,0,0), cv2.FILLED);\n",
    "            cv2.putText(traj, text, (10,50), cv2.FONT_HERSHEY_PLAIN, 1, (255,255,255), 1, 8);\n",
    "            cv2.imshow('map',traj )\n",
    "            cv2.waitKey(50)\n",
    "            #k = cv2.waitKey(1) & 0xFF\n",
    "            #if k == 27: break\n",
    "\n",
    "    #cv2.waitKey(0)\n",
    "    print('Maximum Error: ', maxError)\n",
    "    cv2.imwrite('/jupyter/notebooks/week_9/project/map2.png', traj)\n",
    "  #  imgpts, jac = cv2.projectPoints(pnp_objP, rvec, tvec, K, None)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    left_img    = getLeftImage(0)\n",
    "    right_img   = getRightImage(0)\n",
    "\n",
    "    baseline = 0.54;\n",
    "    K =  getK()\n",
    "\n",
    "    playImageSequence(left_img, right_img, K)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
